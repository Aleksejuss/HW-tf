{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Dataset and Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average_loss': 9881.979, 'label/mean': 4771.938, 'loss': 9881.979, 'prediction/mean': 4736.143, 'global_step': 1000}\n",
      "{'average_loss': 10313.907, 'label/mean': 4734.206, 'loss': 10313.907, 'prediction/mean': 4696.633, 'global_step': 1000}\n",
      "[1048.5254]\n",
      "[2095.5664]\n",
      "[7330.7725]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generating the date\n",
    "x_data = np.random.randint(1000, 8000, 1000000)\n",
    "y_true = x_data + 250\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Defining feature collumns\n",
    "feat_cols = [tf.feature_column.numeric_column('size', shape=[1])]\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=feat_cols, optimizer=tf.keras.optimizers.Adam(learning_rate=0.01))\n",
    "\n",
    "# Splitting the data to train and evaluation data\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(x_data, y_true, test_size=0.3, random_state=101)\n",
    "\n",
    "# Specify batching/shuffling/repeating options with numpy_input_fn\n",
    "input_func = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': x_train}, y_train, batch_size=1, num_epochs=None, shuffle=True)\n",
    "train_input_func = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': x_train}, y_train, batch_size=1, num_epochs=None, shuffle=True)\n",
    "eval_input_func = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': x_eval}, y_eval, batch_size=1, num_epochs=None, shuffle=True)\n",
    "\n",
    "estimator.train(input_fn=input_func, steps=1000)\n",
    "\n",
    "# Get and print the training and evaluation metrics\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_func, steps=500)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_func, steps=500)\n",
    "print(train_metrics)\n",
    "print(eval_metrics)\n",
    "\n",
    "brand_new_data = np.array([1000, 2000, 7000])\n",
    "input_fn_predict = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': brand_new_data}, num_epochs=1, shuffle=False)\n",
    "prediction_result = estimator.predict(input_fn=input_fn_predict)\n",
    "\n",
    "for res in prediction_result:\n",
    "    print(res['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps how to predict prices correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hyperparameters tuning\n",
    "\n",
    "- Increasing/ decreasing `learning rate`\n",
    "- Changing `Optimizers` \n",
    "- Increasing `batch size` \n",
    "- Increasing `epochs` number \n",
    "- Increasing, decreasing estimator `steps` size \n",
    "\n",
    "### 2. Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average_loss': 60558.742, 'label/mean': 4733.352, 'loss': 60558.742, 'prediction/mean': 4487.2676, 'global_step': 1000}\n",
      "{'average_loss': 60553.957, 'label/mean': 4749.742, 'loss': 60553.957, 'prediction/mean': 4503.668, 'global_step': 1000}\n",
      "[1001.7902]\n",
      "[2002.4004]\n",
      "[7005.4517]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generating the date\n",
    "x_data = np.random.randint(1000, 8000, 1000000)\n",
    "y_true = x_data + 250\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Defining feature collumns\n",
    "feat_cols = [tf.feature_column.numeric_column('size', shape=[1])]\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=feat_cols, optimizer=tf.keras.optimizers.Adam(learning_rate=0.00239))\n",
    "\n",
    "\n",
    "# Splitting the data to train and evaluation data\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(x_data, y_true, test_size=0.3, random_state=101)\n",
    "\n",
    "# Specify batching/shuffling/repeating options with numpy_input_fn\n",
    "input_func = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': x_train}, y_train, batch_size=1, num_epochs=None, shuffle=True)\n",
    "train_input_func = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': x_train}, y_train, batch_size=1, num_epochs=None, shuffle=True)\n",
    "eval_input_func = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': x_eval}, y_eval, batch_size=1, num_epochs=None, shuffle=True)\n",
    "\n",
    "estimator.train(input_fn=input_func, steps=1000)\n",
    "\n",
    "# Get and print the training and evaluation metrics\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_func, steps=500)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_func, steps=500)\n",
    "print(train_metrics)\n",
    "print(eval_metrics)\n",
    "\n",
    "brand_new_data = np.array([1000, 2000, 7000])\n",
    "input_fn_predict = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': brand_new_data}, num_epochs=1, shuffle=False)\n",
    "prediction_result = estimator.predict(input_fn=input_fn_predict)\n",
    "\n",
    "for res in prediction_result:\n",
    "    print(res['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Changing Optimizer and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average_loss': 69545.36, 'label/mean': 4759.116, 'loss': 69545.36, 'prediction/mean': 4495.482, 'global_step': 1000}\n",
      "{'average_loss': 69687.79, 'label/mean': 4839.498, 'loss': 69687.79, 'prediction/mean': 4575.602, 'global_step': 1000}\n",
      "[997.8887]\n",
      "[1994.605]\n",
      "[6978.187]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generating the date\n",
    "x_data = np.random.randint(1000, 8000, 1000000)\n",
    "y_true = x_data + 250\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Defining feature collumns\n",
    "feat_cols = [tf.feature_column.numeric_column('size', shape=[1])]\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=feat_cols, optimizer=tf.keras.optimizers.Nadam(learning_rate=0.00239))\n",
    "\n",
    "\n",
    "# Splitting the data to train and evaluation data\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(x_data, y_true, test_size=0.3, random_state=101)\n",
    "\n",
    "# Specify batching/shuffling/repeating options with numpy_input_fn\n",
    "input_func = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': x_train}, y_train, batch_size=1, num_epochs=None, shuffle=True)\n",
    "train_input_func = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': x_train}, y_train, batch_size=1, num_epochs=None, shuffle=True)\n",
    "eval_input_func = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': x_eval}, y_eval, batch_size=1, num_epochs=None, shuffle=True)\n",
    "\n",
    "estimator.train(input_fn=input_func, steps=1000)\n",
    "\n",
    "# Get and print the training and evaluation metrics\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_func, steps=500)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_func, steps=500)\n",
    "print(train_metrics)\n",
    "print(eval_metrics)\n",
    "\n",
    "brand_new_data = np.array([1000, 2000, 7000])\n",
    "input_fn_predict = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': brand_new_data}, num_epochs=1, shuffle=False)\n",
    "prediction_result = estimator.predict(input_fn=input_fn_predict)\n",
    "\n",
    "for res in prediction_result:\n",
    "    print(res['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing batch size, epochs, steps, learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average_loss': 64073.938, 'label/mean': 4815.43, 'loss': 64073.938, 'prediction/mean': 4562.3086, 'global_step': 820}\n",
      "{'average_loss': 64090.656, 'label/mean': 4851.206, 'loss': 64090.656, 'prediction/mean': 4598.055, 'global_step': 820}\n",
      "[1000.1244]\n",
      "[1999.214]\n",
      "[6994.6616]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generating the date\n",
    "x_data = np.random.randint(1000, 8000, 1000000)\n",
    "y_true = x_data + 250\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Defining feature collumns\n",
    "feat_cols = [tf.feature_column.numeric_column('size', shape=[1])]\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=feat_cols, optimizer=tf.keras.optimizers.Adam(learning_rate=0.00239))\n",
    "\n",
    "\n",
    "# Splitting the data to train and evaluation data\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(x_data, y_true, test_size=0.3, random_state=101)\n",
    "\n",
    "# Specify batching/shuffling/repeating options with numpy_input_fn\n",
    "input_func = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': x_train}, y_train, batch_size=10, num_epochs=2, shuffle=True)\n",
    "train_input_func = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': x_train}, y_train, batch_size=1, num_epochs=None, shuffle=True)\n",
    "eval_input_func = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': x_eval}, y_eval, batch_size=1, num_epochs=None, shuffle=True)\n",
    "\n",
    "estimator.train(input_fn=input_func, steps=820)\n",
    "\n",
    "# Get and print the training and evaluation metrics\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_func, steps=500)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_func, steps=500)\n",
    "print(train_metrics)\n",
    "print(eval_metrics)\n",
    "\n",
    "brand_new_data = np.array([1000, 2000, 7000])\n",
    "input_fn_predict = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': brand_new_data}, num_epochs=1, shuffle=False)\n",
    "prediction_result = estimator.predict(input_fn=input_fn_predict)\n",
    "\n",
    "for res in prediction_result:\n",
    "    print(res['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying with data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this example i am not sure should i denormalized data in the end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average_loss': 4.2054263e-10, 'label/mean': -0.10399162, 'loss': 4.2054263e-10, 'prediction/mean': -0.103987165, 'global_step': 500}\n",
      "{'average_loss': 3.9006845e-10, 'label/mean': 0.017260196, 'loss': 3.9006845e-10, 'prediction/mean': 0.01726218, 'global_step': 500}\n",
      "[999.9796]\n",
      "[1999.9592]\n",
      "[6999.8574]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "# Generating the date\n",
    "x_data = np.random.randint(1000, 8000, 1000000)\n",
    "y_true = x_data + 250\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Defining feature collumns\n",
    "feat_cols = [tf.feature_column.numeric_column('size', shape=[1])]\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=feat_cols, optimizer=tf.keras.optimizers.Adam(learning_rate=0.01))\n",
    "\n",
    "# Splitting the data to train and evaluation data\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(x_data, y_true, test_size=0.3, random_state=101)\n",
    "\n",
    "\n",
    "#data normalizing\n",
    "norm = StandardScaler()\n",
    "\n",
    "x_train_norm = normalize(x_train).reshape(-1, 1)\n",
    "x_eval_norm = normalize(x_eval).reshape(-1, 1)\n",
    "y_train_norm = normalize(y_train).reshape(-1, 1)\n",
    "y_eval_norm = normalize(y_eval).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Specify batching/shuffling/repeating options with numpy_input_fn\n",
    "input_func = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': x_train_norm}, y_train_norm, batch_size=1, num_epochs=None, shuffle=True)\n",
    "train_input_func = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': x_train_norm}, y_train_norm, batch_size=1, num_epochs=None, shuffle=True)\n",
    "eval_input_func = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': x_eval_norm}, y_eval_norm, batch_size=1, num_epochs=None, shuffle=True)\n",
    "\n",
    "estimator.train(input_fn=input_func, steps=500)\n",
    "\n",
    "# Get and print the training and evaluation metrics\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_func, steps=500)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_func, steps=500)\n",
    "print(train_metrics)\n",
    "print(eval_metrics)\n",
    "\n",
    "brand_new_data = np.array([1000, 2000, 7000])\n",
    "input_fn_predict = tf.compat.v1.estimator.inputs.numpy_input_fn({'size': brand_new_data}, num_epochs=1, shuffle=False)\n",
    "prediction_result = estimator.predict(input_fn=input_fn_predict)\n",
    "\n",
    "for res in prediction_result:\n",
    "    print(res['predictions'])\n",
    "#     print(res['predictions'] * x_data.std() + x_data.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition\n",
    "### Keras implementation\n",
    "\n",
    "\n",
    "In this example also i am not sure should i denormalized data in the end?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.random.randint(1000, 8000, 1000000)\n",
    "y_true = x_data + 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_true.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1,input_dim=1))\n",
    "model.add(Dense(1,activation='linear'))\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.1), loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = StandardScaler()\n",
    "x_data_norm = norm.fit_transform(x_data)\n",
    "y_data_norm = norm.fit_transform(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000/100000 [==============================] - 36s 356us/step - loss: 6.8805e-04 - mse: 6.8805e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d00dc0c160>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_data_norm, y_data_norm, batch_size=10, epochs=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 999.9919]\n",
      " [1999.9839]\n",
      " [6999.944 ]]\n"
     ]
    }
   ],
   "source": [
    "brand_new_data = np.array([1000, 2000, 7000])\n",
    "pred = model.predict(brand_new_data)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
